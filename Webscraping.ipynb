{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML file detected.\n"
     ]
    }
   ],
   "source": [
    "#Locates user's comments and cleans up html\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "Tk().withdraw()\n",
    "filename = askopenfilename()\n",
    "\n",
    "if filename.endswith('.html'):\n",
    "    print(\"HTML file detected.\")\n",
    "else:\n",
    "    print(\"This is not an HTML file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify comment nests, include upvote count\n",
    "#Publish results to CSV file in folder\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, timedelta\n",
    "import html5lib\n",
    "import csv\n",
    "\n",
    "#prepare export CSV file\n",
    "today = date.today().strftime('%Y-%m-%d')\n",
    "new_csv_file_name = f'C:/Users/p40014d/OneDrive - AholdDelhaize.com/Documents/Web Scraping/Reddit - Walmart Theft Article Comments/Reddit Scrape Results {today}.csv'\n",
    "with open(new_csv_file_name, 'w', encoding = 'utf8', newline = '') as csv_file:\n",
    "    csv_write = csv.writer(csv_file)\n",
    "    with open(filename, 'r', encoding = 'utf8') as html_file:\n",
    "        content = html_file.read()\n",
    "\n",
    "        soup = BeautifulSoup(content, 'html5lib')\n",
    "        comments_cards = soup.find_all('div', class_ = '_3tw__eCCe7j-epNCKGXUKk') #.find only finds first element, use .find_all\n",
    "\n",
    "        for commenter in comments_cards:\n",
    "            # pull data from website\n",
    "            # struggling to pull usernames. BS is not detecting any other URL than days\n",
    "            time_since = commenter.find_all('a')[0].text\n",
    "            \n",
    "            #comment = commenter.find('p', class_='_1qeIAgB0cPwnLhDF9XSiJM')\n",
    "            comment = commenter.find('div', class_=\"_292iotee39Lmt0MkQZ2hPV RichTextJSON-root\")\n",
    "            score = commenter.find('div', class_='_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M').text \n",
    "            level = commenter.find('span', class_='_1RIl585IYPW6cmNXwgRz0J').text\n",
    "\n",
    "            # navigate Reddit's method of communicating dates\n",
    "            if \"day\" in time_since:\n",
    "                num_days = int(time_since[0])\n",
    "                post_date = date.today() - timedelta(days=num_days)\n",
    "            elif \"month\" in time_since:\n",
    "                try:\n",
    "                    num_mon = int(time_since[0])\n",
    "                except: \n",
    "                    num_mon = 1\n",
    "                post_date = date.today() - timedelta(months=num_mon)\n",
    "            elif \"year\" in time_since:\n",
    "                try:\n",
    "                    num_year = int(time_since[0])\n",
    "                except: \n",
    "                    num_year = 1\n",
    "                post_date = date.today() - timedelta(years=num_year)\n",
    "            else: post_date = 0\n",
    "\n",
    "            # converts score to a useable number\n",
    "            if score[-1] == \"k\":\n",
    "                score1 = score.rstrip(score[-1])\n",
    "                score1 = float(score1) * 1000\n",
    "            else:\n",
    "                score1 = int(score)\n",
    "\n",
    "            # try to write a CSV file with new data\n",
    "            try:\n",
    "                csv_write.writerow([comment.text, score1, level, post_date]) #needs brackets to keep strings combined\n",
    "            except PermissionError as err:\n",
    "                print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m comments_cards \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msitetable nestedlisting\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m#.find only finds first element, use .find_all\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m commenter \u001b[39min\u001b[39;00m comments_cards:\n\u001b[1;32m---> 21\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39;49mfindall(\u001b[39m'\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m'\u001b[39;49m):\n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m i\u001b[39m.\u001b[39mhas_attr(\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     23\u001b[0m             time_since \u001b[39m=\u001b[39m i[\u001b[39m'\u001b[39m\u001b[39mdatetime\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "#Identify comment nests, include upvote count\n",
    "#Publish results to CSV file in folder\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, timedelta\n",
    "import html5lib\n",
    "import csv\n",
    "\n",
    "#prepare export CSV file\n",
    "today = date.today().strftime('%Y-%m-%d')\n",
    "new_csv_file_name = f'C:/Users/p40014d/OneDrive - AholdDelhaize.com/Documents/Web Scraping/Reddit - Walmart Theft Article Comments/Reddit Scrape Results {today}.csv'\n",
    "with open(new_csv_file_name, 'w', encoding = 'utf8', newline = '') as csv_file:\n",
    "    csv_write = csv.writer(csv_file)\n",
    "    with open(filename, 'r', encoding = 'utf8') as html_file:\n",
    "        content = html_file.read()\n",
    "\n",
    "        soup = BeautifulSoup(content, 'html5lib')\n",
    "        comments_cards = soup.find('div', class_ = 'sitetable nestedlisting') #.find only finds first element, use .find_all\n",
    "\n",
    "        for commenter in comments_cards:\n",
    "           \n",
    "            for i in soup.findall('time'):\n",
    "                if i.has_attr('datetime'):\n",
    "                    time_since = i['datetime']\n",
    "            \n",
    "            #comment = commenter.find('p', class_='_1qeIAgB0cPwnLhDF9XSiJM')\n",
    "            username = commenter.find('a', class_=\"author may-blank id-t2_8e7zd\")\n",
    "            comment = commenter.find('div', class_=\"usertext-body may-blank-within md-container \")\n",
    "            score = commenter.find('span', class_='score unvoted').text \n",
    "            #level = commenter.find('span', class_='_1RIl585IYPW6cmNXwgRz0J').text\n",
    "\n",
    "            # converts score to a useable number\n",
    "            if score[-1] == \"k\":\n",
    "                score1 = score.rstrip(score[-1])\n",
    "                score1 = float(score1) * 1000\n",
    "            else:\n",
    "                score1 = int(score)\n",
    "            \n",
    "            print(username)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Scaper, w/HTML pull request, no file necessary\n",
    "#certification necessary for https\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://old.reddit.com/r/news/comments/zf54gb/retail_theft_at_walmart_may_lead_to_raised_prices/?sort=top/.json'\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"q\": \"flair:prompt\",\n",
    "    \"restrict_sr\": \"1\",\n",
    "    \"sr_nsfw\": \"\",\n",
    "    \"t\": \"all\",\n",
    "    \"sort\": \"top\",\n",
    "}\n",
    "\n",
    "data = requests.get(\n",
    "    url,\n",
    "    params=params,\n",
    "    headers=headers,\n",
    ").json()\n",
    "\n",
    "try:\n",
    "    r = requests.get(url, headers = headers)\n",
    "except requests.exceptions.SSLError as err:\n",
    "    print('SSL Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify comment nests, include upvote count\n",
    "#Publish results to CSV file in folder\n",
    "# trying something other than BS\n",
    "\n",
    "import lxml.html\n",
    "from datetime import date, timedelta\n",
    "import csv\n",
    "\n",
    "#prepare export CSV file\n",
    "today = date.today().strftime('%Y-%m-%d')\n",
    "new_csv_file_name = f'C:/Users/p40014d/OneDrive - AholdDelhaize.com/Documents/Web Scraping/Reddit - Walmart Theft Article Comments/Reddit Scrape Results {today}.csv'\n",
    "\n",
    "with open(filename, 'r', encoding = 'utf8') as html_file:\n",
    "    doc = html_file.read()\n",
    "    content = lxml.html.parse(doc)\n",
    "\n",
    "    comments_cards = content.findall('div', class_ = '_3tw__eCCe7j-epNCKGXUKk') #.find only finds first element, use .find_all\n",
    "\n",
    "    for commenter in comments_cards:\n",
    "        # pull data from website\n",
    "        # struggling to pull usernames. BS is not detecting any other URL than days\n",
    "        time_since = commenter.findall('a')[0].text\n",
    "            \n",
    "        #comment = commenter.find('p', class_='_1qeIAgB0cPwnLhDF9XSiJM')\n",
    "        comment = commenter.find('div', class_=\"_292iotee39Lmt0MkQZ2hPV RichTextJSON-root\")\n",
    "        score = commenter.find('div', class_='_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M').text \n",
    "        level = commenter.find('span', class_='_1RIl585IYPW6cmNXwgRz0J').text\n",
    "\n",
    "        # navigate Reddit's method of communicating dates\n",
    "        if \"day\" in time_since:\n",
    "            num_days = int(time_since[0])\n",
    "            post_date = date.today() - timedelta(days=num_days)\n",
    "        elif \"month\" in time_since:\n",
    "            try:\n",
    "                num_mon = int(time_since[0])\n",
    "            except: \n",
    "                num_mon = 1\n",
    "            post_date = date.today() - timedelta(months=num_mon)\n",
    "        elif \"year\" in time_since:\n",
    "            try:\n",
    "                    num_year = int(time_since[0])\n",
    "            except: \n",
    "                num_year = 1\n",
    "            post_date = date.today() - timedelta(years=num_year)\n",
    "        else: post_date = 0\n",
    "\n",
    "        # converts score to a useable number\n",
    "        if score[-1] == \"k\":\n",
    "            score1 = score.rstrip(score[-1])\n",
    "            score1 = float(score1) * 1000\n",
    "        else:\n",
    "            score1 = int(score)\n",
    "        \n",
    "        print(comment)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c4a26ec77b70a0de623b940775a788e88d8a6e9f3ab87532893bafb7eb31bc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
